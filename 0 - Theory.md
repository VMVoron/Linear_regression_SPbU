# Регресионный анализ
## Допущения (assumptions) регрессионного анализа
1. Переменные модели должны иметь распределение, близкое к 
нормальному.
2. Зависимая и независимые переменные должны быть измерены в 
метрической шкале.
3. Для построения линейных регрессий, зависима и независимые 
переменные должны иметь линейную связь.
4. Отсутствие мультиколлинеарности – независимость между собой 
переменных-предикторов, отсутствие высокой корреляции (для 
множественной регрессии). Решение: удаление высоко коррелируемых 
переменных из анализа или центрирование данных (вычитание 
средних значений из каждого наблюдения по необходимым 
переменным).
5. Отсутствие автокорреляции – отсутствие независимости остатков. 
Выявляется с помощью теста Дурбина-Уотсона (обнаруживает 
автокорреляцию первого порядка). 
‒ Если d=0 – полная положительная автокорреляция
‒ Если d=4 – полная отрицательная автокорреляция
‒ Если d=2 – отсутствие автокорреляции
6. Гомоскедастичность - дисперсия остатков одинакова для каждого 
значения. Определяется с помощью диаграммы рассеяния.

## Регрессионный анализ: основные положения
Регрессионный анализ – это инструмент для количественного определения 
значения одной переменной на основании другой. 

Парная (простая) линейная регрессия даёт нам правила, определяющие линию 
регрессии, которая лучше других предсказывает наиболее вероятные значения 
одной переменной на основании другой (переменных всего две). 

Множественная регрессия является расширением простой линейной регрессии. 

По оси Y располагают переменную, которую необходимо предсказать (зависимую), 
а по оси Х – переменную, на основе которой будет осуществляться предсказание 
(независимую). 

Зависимая переменная – это переменная в регрессии, которую нельзя изменять, 
её изменение является следствием влияния независимой переменной 
(переменных). 

Независимая переменная – это та переменная в регрессии, которую можно 
изменять. 

Коэффициенты регрессии (β) — это коэффициенты, которые рассчитываются в
результате выполнения регрессионного анализа. Вычисляются величины для 
каждой независимой переменной, которые представляют силу и тип взаимосвязи 
независимой переменной по отношению к зависимой.

Мультиколлинеарность (multicollinearity) — в эконометрике (регрессионный анализ) — наличие линейной зависимости между объясняющими переменными (факторами) регрессионной модели. При этом различают полную коллинеарность, которая означает наличие функциональной (тождественной) линейной зависимости и частичную или просто мультиколлинеарность — наличие сильной корреляции между факторами.

Полная коллинеарность приводит к неопределенности параметров в линейной регрессиионной модели независимо от методов оценки.

Гомоскедастичность (англ. homoscedasticity) — свойство, означающее постоянство условной дисперсии вектора или последовательности случайных величин. Однородная вариативность значений наблюдений, выражающаяся в стабильности, однородности дисперсии случайной ошибки регрессионной модели — дисперсии одинаковы во все моменты измерения. Противоположное явление носит название гетероскедастичности. Является обязательным условием применения метода наименьших квадратов.

Гетероскедастичность (англ. heteroscedasticity) — понятие, используемое в прикладной статистике (чаще всего — в эконометрике), означающее неоднородность наблюдений, выражающуюся в неодинаковой (непостоянной) дисперсии случайной ошибки регрессионной (эконометрической) модели. Гетероскедастичность противоположна гомоскедастичности, означающей однородность наблюдений, то есть постоянство дисперсии случайных ошибок модели
