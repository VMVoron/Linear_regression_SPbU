Прочитаем данные в виде таблицы
```{r}
data <- read.table('mortality.txt',             
                   header = TRUE) 
```
Так как факторов много, чтобы не идти вслепую, посмотрим сразу, \
присутствует ли в датасете мультиколлинеарность через корреляционную матрицу
```{r}
library(corrplot)
corr<-cor(data)
corrplot(corr)
```
![png](https://github.com/VMVoron/Linear_regression_SPbU/blob/main/Rplot.png)

Как видим, переменные HC ~ NOX коррелируют друг с другом очень сильно, их лучше исключить из анализа
Также можно отметить такие переменные, как: nonw ~ poor, educ ~ wwdrk, nonw ~ mortality

Итак, вероятно, следующие ожидаемые переменные больше всего влияют на уровень смертности: 
> annual_precipitation (prec), \
> popn(the number of members per household) \
> the size of the nonwhite population (nonw) \
> the number of families with an income less than $3000; (poor) \ 
> и отдельно загрязнения (hc, noc, so2) 

Однако, эти данные получены "на глаз", построим более подробный график с коэффициентами


Создадим график, содержащий фактические коэффициенты линейной корреляции для каждой пары переменных:
```{r}
#install and load the *GGally* library
install.packages(GGally)
library(GGally)
#generate the pairs plot
ggpairs(data)
```
![png](https://github.com/VMVoron/Linear_regression_SPbU/blob/main/Rplot01.png)

### Из графика линейной корреляции выделим следующие предикторы, которые коррелируют с переменной mort:
1. nonw - 0.644 - коррелирует с poor
2. educ -0.511 - коррелирует c wwdrk 
3. prec 0.509
4. hous -0.427 - коррелирует с educ
5. so2 0.426
6. poor 0.41 - коррелирует с nonw 
7. popn 0.357
8. wwdrk -0.284  - коррелирует с educ
9. jult 0.278
10. dens 0.265


Возьмем данные целиком, чтобы понять, с чем мы имеем дело в цифрах и построим регрессию со всеми изначальными предикторами


```{r}
model1$coefficients
```

```{r}
  (Intercept)          prec          jant          jult         ovr65          popn          educ          hous          dens          nonw         wwdrk 
1863.15733425    2.07239872   -2.17756526   -2.83377840  -14.04208883 -115.43205477  -24.24708231   -1.14602913    0.01004162    3.53323346    0.52292967 
         poor            hc           nox           so2         humid 
   0.26706708   -0.88901097    1.86641266   -0.03447204    0.53310932 

```
Получена линейная модель вида: 
Уровень смертности = 1863 + 2.07 * prec - 2.17 * jant - 2.833 * jult - 14.04 * ovr65 - 115.43 * popn - 24.24 * educ - 1.14 * hous + 0.01 * dens + 3.53 * nonw + 0.5 * wwdrk + 0.26* poor - 0.88 * hc + 1.88 * nox - 0.03 * so2 + 0.53 * humid


```{r}
min(data$mort)
max(data$mort)
```
```{r}
[1] 790.733
[1] 1113.156
```
Из таблицы Residuals:  
- Наибольшее отрицательное истнное отклонение от модельного -   -72.285 
- Наибольшее отрицательное истнное отклонение от модельного -   75.586  
- При минимальном значении ряда -  790.733, а максимальном - 1113.156

```{r}
model1 <- lm(mort~., data=data)
summary(model1)
```
```{r}
Call:
lm(formula = mort ~ ., data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-75.285 -14.640   0.694  14.790  75.586 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.863e+03  4.108e+02   4.535  4.4e-05 ***
prec         2.072e+00  8.418e-01   2.462  0.01781 *  
jant        -2.178e+00  6.752e-01  -3.225  0.00238 ** 
jult        -2.834e+00  1.771e+00  -1.600  0.11670    
ovr65       -1.404e+01  7.746e+00  -1.813  0.07670 .  
popn        -1.154e+02  6.200e+01  -1.862  0.06933 .  
educ        -2.425e+01  1.121e+01  -2.163  0.03605 *  
hous        -1.146e+00  1.467e+00  -0.781  0.43871    
dens         1.004e-02  4.123e-03   2.435  0.01899 *  
nonw         3.533e+00  1.282e+00   2.755  0.00850 ** 
wwdrk        5.229e-01  1.551e+00   0.337  0.73760    
poor         2.671e-01  2.565e+00   0.104  0.91755    
hc          -8.890e-01  4.524e-01  -1.965  0.05574 .  
nox          1.866e+00  9.345e-01   1.997  0.05201 .  
so2         -3.447e-02  1.423e-01  -0.242  0.80968    
humid        5.331e-01  1.052e+00   0.507  0.61474    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 32.33 on 44 degrees of freedom
Multiple R-squared:  0.7985,	Adjusted R-squared:  0.7298 
F-statistic: 11.63 on 15 and 44 DF,  p-value: 9.56e-11
```

Несмотря на то, что много предикторов в модели не являются статистически значимыми, мы получили значние среднеквадратичного отклонения - 0.7298 \
Это означает, что уровень смертности почти на 80% определен предикторвами  внутри полученной модели. 
F stat - 11.63 - это больше табличного двух с небольшим  ( степени свободы 15-44, при уровне значимости - 0,05) \
Это лишь подтверждаем значимость среднеквадратичного отклонения
![png](https://github.com/VMVoron/Linear_regression_SPbU/blob/main/F.gif)

Когда мы взяли факторы не попарно, а все вместе, мы выявили слеюущие статистически значимые факторы при уровне значимости <0.05 \
( в выводе они помечены звездочками и точками при разных уровнях значимости) :
1. nonw
2. jant
3. prec
4. educ
5. dens

Но не факт, что если именно только  эти факторы мы будем использовать в модели, то ои будут давать наилучшую комбинацию.

Сравним эти данные с данными из графика линейной корреляции с зависимой переменной:
### Из графика линйной корреляции выделим следующие предикторы, которые коррелируют с переменной mort:
1. nonw - 0.644 - коррелирует с poor 0.7
2. educ -0.511 - коррелирует c wwdrk  0.703
3. prec 0.509
4. hous -0.427 - коррелирует с educ 0.552
5. so2 0.426
6. poor 0.41 - коррелирует с nonw 0.705
7. popn 0.357 - коррелирует с nonw 0.419
8. wwdrk -0.284  - коррелирует с educ 0.703
9. jult 0.278
10. dens 0.265


Практически все из статистически значимых переменных присутствовали в графике линейной корреляции с высоикми показателями. Каким-то образом затесалась переменная jant, видимо, на неё повлияло наличие других переменных, так как сама по себе она не оказывает прямого влияния на mort


Если бы мы отбирали переменные вручную для построения модели, какие переменные мы бы взяли?
Буду отбирать те переменные, у которых нет большой положительной связи между собой (> 0.4)
1. nonw
2. educ
3. prec
4. hous не берём, так как есть корреляция с educ 
5. so2
6. poor не берем, т.к. коррелция с nonw
7. popn не берем, т.к. корреляция с nonw
8. wwdrk не берем, т.к. корллеяция с educ
9. jult
10. dens

Ручным методом отобрали 6 объясняющих переменных: 
1. nonw
2. educ
3. prec
4. so2
5. jult
6. dens


Принимая во внимание всё вышеуказанное, можно выделить выборочно один фактор, более всего влияющий на модель: \
принадлежность к people of color - nonwhite

Однако, зачем мучаться и вручную просматривать переменные, если можно обратиться к автоматизированному выбору переменных? Например, существует метод перебора всех переменных с целью уменьшения среднеквадратической ошибки (MSE)
Пример отбора переменных с использованием MSE [тут](https://github.com/VMVoron/Linear_regression_SPbU/blob/main/2-MSE.md)
